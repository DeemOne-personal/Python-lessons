{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "colab": {
      "name": "Copy of homework_2_poetry_generation_TEST.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeemOne-personal/Python-lessons/blob/main/Copy_of_homework_2_poetry_generation_TEST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Quao1WwFPOvn"
      },
      "source": [
        "## Homework №2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txzufAHoPOvo"
      },
      "source": [
        "### Almost Shakespeare\n",
        "\n",
        "Let's try to generate some Shakespeare poetry using RNNs. The sonnets file is available in the notebook directory.\n",
        "\n",
        "Text generation can be designed in several steps:\n",
        "    \n",
        "1. Data loading.\n",
        "2. Dictionary generation.\n",
        "3. Data preprocessing.\n",
        "4. Model (neural network) training.\n",
        "5. Text generation (model evaluation).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUPTdS1BPOvp"
      },
      "source": [
        "### Data loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KyYA5dZPOvp"
      },
      "source": [
        "Shakespeare sonnets are awailable at this [link](http://www.gutenberg.org/ebooks/1041?msg=welcome_stranger). In addition, they are stored in the same directory as this notebook (`sonnetes.txt`).\n",
        "\n",
        "Simple preprocessing is already done for you in the next cell: all technical info is dropped."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8rI-T6qjkly"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "hERfiZL8POvq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f120622-cec4-4095-c27c-f1cb68a27b7a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#with open('/content/drive/MyDrive/Colab Notebooks/Edu4 - DS MFTI Adv/Files/HW2/sonnets.txt', 'r') as iofile:\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/Edu4 - DS MFTI Adv/Files/HW2/onegin.txt', 'r') as iofile:\n",
        "    source_text = iofile.readlines()\n",
        "    \n",
        "#TEXT_START = 45\n",
        "#TEXT_END = -368\n",
        "names = source_text[TEXT_START : TEXT_END]\n",
        "#assert len(names) == 2616\n",
        "start_token = \" \""
      ],
      "execution_count": 342,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aR4As896-ce-"
      },
      "source": [
        "import re\n",
        "fullstr=''.join(line for line in names).lower()\n",
        "fullstr = re.sub(r'[^\\w]', ' ', fullstr) #remove extra characters\n",
        "fullstr=re.sub(' +', ' ', fullstr) #remove extra characters\n",
        "names=fullstr"
      ],
      "execution_count": 343,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1j7isOykPOvq"
      },
      "source": [
        "In opposite to the in-class practice, this time we want to predict complex text. Let's reduce the complexity of the task and lowercase all the symbols.\n",
        "\n",
        "Now variable `text` is a list of strings. Join all the strings into one and lowercase it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZjUQri8POvr"
      },
      "source": [
        "Put all the characters, that you've seen in the text, into variable `tokens`.\n",
        "\n",
        "Create dictionary `token_to_idx = {<char>: <index>}` and dictionary `idx_to_token = {<index>: <char>}`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxuhyqyjqPrh",
        "outputId": "d4f05c2f-1552-4d17-bfab-0be144d08443"
      },
      "source": [
        "tokens = set()\n",
        "\n",
        "for name in names:\n",
        "    tokens.update(set(name))\n",
        "tokens = sorted(list(tokens))  # <list of all unique characters in the dataset>\n",
        "\n",
        "num_tokens = len(tokens)\n",
        "print(\"num_tokens = \", num_tokens)\n",
        "\n",
        "# <dictionary of symbol -> its identifier (index in tokens list)>\n",
        "token_to_id = {token: idx for idx, token in enumerate(tokens)}\n",
        "\n",
        "assert len(tokens) == len(token_to_id), \"dictionaries must have same size\"\n",
        "\n",
        "for i in range(num_tokens):\n",
        "    assert token_to_id[tokens[i]] == i, \"token identifier must be it's position in tokens list\"\n"
      ],
      "execution_count": 344,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num_tokens =  60\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUaKqXI8L9j9",
        "outputId": "2678dbd0-84c1-43cb-e1ff-faa801884505"
      },
      "source": [
        "#temp\n",
        "#num_tokens = len(tokens)\n",
        "#print(\"num_tokens = \", num_tokens)\n",
        "#idx_to_token={k:v for k, v in enumerate(tokens)}\n",
        "#token_to_idx = {token: idx for idx, token in enumerate(tokens)}\n",
        "#assert len(idx_to_token) == len(token_to_idx), \"dictionaries must have same size\""
      ],
      "execution_count": 340,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num_tokens =  60\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsCjlbKuMhuU"
      },
      "source": [
        "#def encode(str):\n",
        "#  return np.array([token_to_idx[ch] for ch in str])\n",
        "\n",
        "#def decode(str):\n",
        "#  return np.array([idx_to_token[ch] for ch in str])"
      ],
      "execution_count": 259,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxpgxWBwM-9-",
        "outputId": "a7aa18fa-5b02-40bd-afd2-c237bbbde1a1"
      },
      "source": [
        "#print(encode(['a','b','c']))\n",
        "#print(deecode([2,3,4]))"
      ],
      "execution_count": 266,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2 3 4]\n",
            "['a' 'b' 'c']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-j5uf_-POvt"
      },
      "source": [
        "### Building the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4go5JugdPOvt"
      },
      "source": [
        "Now we want to build and train recurrent neural net which would be able to something similar to Shakespeare's poetry.\n",
        "\n",
        "Let's use vanilla RNN, similar to the one created during the lesson."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N58SOXVCk21L"
      },
      "source": [
        "import numpy as np\n",
        "def to_matrix(names, max_len=None, pad=token_to_id[\" \"], dtype=\"int32\", batch_first=True):\n",
        "    \"\"\"Casts a list of names into rnn-digestable matrix\"\"\"\n",
        "\n",
        "    names=list([names])\n",
        "\n",
        "    max_len = max_len or max(map(len, names))\n",
        "    names_ix = np.zeros([len(names), max_len], dtype) + pad\n",
        "\n",
        "    for i in range(len(names)):\n",
        "        line_ix = [token_to_id[c] for c in names[i]]\n",
        "        names_ix[i, : len(line_ix)] = line_ix\n",
        "\n",
        "    if not batch_first:  # convert [batch, time] into [time, batch]\n",
        "        names_ix = np.transpose(names_ix)\n",
        "\n",
        "    return names_ix"
      ],
      "execution_count": 345,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPA4ejPOO7QA"
      },
      "source": [
        "#TEMP funct\n",
        "#import torch\n",
        "#import torch.nn.functional as F\n",
        "\n",
        "#def to_matrix(names, max_len=None, pad=token_to_id[\" \"], dtype=\"int32\", batch_first=True):\n",
        "#def getmatrix(strr):\n",
        "#  strr=names\n",
        "#  num_classes=num_tokens+1\n",
        "#  encoded=encode(strr)\n",
        "##  x = torch.tensor(encoded)\n",
        " # res=F.one_hot(x, num_classes)\n",
        " # return res"
      ],
      "execution_count": 301,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvqJ8v-STJVm",
        "outputId": "be15a269-ec5b-43f4-9a24-7e83b62b340c"
      },
      "source": [
        "to_matrix(names).shape"
      ],
      "execution_count": 322,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([125844, 61])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 322
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n45ATG_hlIcU"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "\n",
        "class CharRNNCell(nn.Module):\n",
        "    \"\"\"\n",
        "    Implement the scheme above as torch module\n",
        "    \"\"\"\n",
        "\n",
        "    #def __init__(self, num_tokens=len(tokens), embedding_size=16, rnn_num_units=64):\n",
        "    def __init__(self, num_tokens=len(tokens), embedding_size=32, rnn_num_units=128):\n",
        "        super(self.__class__, self).__init__()\n",
        "        self.num_units = rnn_num_units\n",
        "\n",
        "        self.embedding = nn.Embedding(num_tokens, embedding_size)\n",
        "        self.rnn_update = nn.Linear(embedding_size + rnn_num_units, rnn_num_units)\n",
        "        self.rnn_to_logits = nn.Linear(rnn_num_units, num_tokens)\n",
        "\n",
        "    def forward(self, x, h_prev):\n",
        "        \"\"\"\n",
        "        This method computes h_next(x, h_prev) and log P(x_next | h_next)\n",
        "        We'll call it repeatedly to produce the whole sequence.\n",
        "\n",
        "        :param x: batch of character ids, containing vector of int64\n",
        "        :param h_prev: previous rnn hidden states, containing matrix [batch, rnn_num_units] of float32\n",
        "        \"\"\"\n",
        "        # get vector embedding of x\n",
        "        # batch, seq leng, emb dim\n",
        "        x_emb = self.embedding(x)\n",
        "\n",
        "        # compute next hidden state using self.rnn_update\n",
        "        # hint: use torch.cat(..., dim=...) for concatenation\n",
        "        x_and_h = torch.cat([x_emb, h_prev], dim=-1)  # YOUR CODE HERE\n",
        "        h_next = self.rnn_update(x_and_h)  # YOUR CODE HERE\n",
        "\n",
        "        h_next = torch.tanh(h_next)  # YOUR CODE HERE\n",
        "\n",
        "        assert h_next.size() == h_prev.size()\n",
        "\n",
        "        # compute logits for next character probs\n",
        "        logits = self.rnn_to_logits(h_next)  # YOUR CODE\n",
        "\n",
        "        return h_next, logits\n",
        "\n",
        "    def initial_state(self, batch_size):\n",
        "        \"\"\" return rnn state before it processes first input (aka h0) \"\"\"\n",
        "        return torch.zeros(batch_size, self.num_units, requires_grad=True)"
      ],
      "execution_count": 346,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUitsztplRqT"
      },
      "source": [
        "char_rnn = CharRNNCell()\n",
        "criterion = nn.NLLLoss()  # YOUR CODE HERE"
      ],
      "execution_count": 347,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-tbIPDQpHAE"
      },
      "source": [
        "**RNN LOOP**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0n2YPF1lVh8"
      },
      "source": [
        "def rnn_loop(char_rnn, batch_ix):\n",
        "    \"\"\"\n",
        "    Computes log P(next_character) for all time-steps in names_ix\n",
        "    :param names_ix: an int32 matrix of shape [batch, time], output of to_matrix(names)\n",
        "    \"\"\"\n",
        "    batch_size, max_length = batch_ix.size()\n",
        "    hid_state = char_rnn.initial_state(batch_size)\n",
        "    logprobs = []\n",
        "\n",
        "    for x_t in batch_ix.transpose(0, 1):\n",
        "        hid_state, logits = char_rnn(x_t, hid_state)  # <-- here we call your one-step code\n",
        "        logprobs.append(F.log_softmax(logits, -1))\n",
        "\n",
        "    return torch.stack(logprobs, dim=1)"
      ],
      "execution_count": 348,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EK1qHZQslYMl",
        "outputId": "61e0c3bc-5860-4141-feec-cfe0a782b1ef"
      },
      "source": [
        "#batch_ix = to_matrix(names[:5])\n",
        "batch_ix = to_matrix(names[:200]) #increased for generating sentences\n",
        "batch_ix = torch.tensor(batch_ix, dtype=torch.int64)\n",
        "\n",
        "logp_seq = rnn_loop(char_rnn, batch_ix)\n",
        "\n",
        "assert torch.max(logp_seq).data.numpy() <= 0\n",
        "assert tuple(logp_seq.size()) == batch_ix.shape + (num_tokens,)\n",
        "logp_seq.shape"
      ],
      "execution_count": 349,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 200, 60])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 349
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H35k_crso_Db"
      },
      "source": [
        "Likelihood and gradients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-tUOuF7ll8e"
      },
      "source": [
        "predictions_logp = logp_seq[:, :-1]\n",
        "actual_next_tokens = batch_ix[:, 1:]\n",
        "\n",
        "# .contiguous() method checks that tensor is stored in the memory correctly to\n",
        "# get its view of desired shape.\n",
        "\n",
        "loss = criterion(\n",
        "    predictions_logp.contiguous().view(-1, num_tokens),\n",
        "    actual_next_tokens.contiguous().view(-1),\n",
        ")"
      ],
      "execution_count": 350,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxYh6XRllowH"
      },
      "source": [
        "loss.backward()"
      ],
      "execution_count": 351,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sx9jEfoKlqaB"
      },
      "source": [
        "for w in char_rnn.parameters():\n",
        "    assert (\n",
        "        w.grad is not None and torch.max(torch.abs(w.grad)).data.numpy() != 0\n",
        "    ), \"Loss is not differentiable w.r.t. a weight with shape %s. Check forward method.\" % (\n",
        "        w.size(),\n",
        "    )"
      ],
      "execution_count": 352,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38fxOoVjmed4"
      },
      "source": [
        "**Loop**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGRv-yiVSkLe"
      },
      "source": [
        "import random\n",
        "def getrndstr(srcstr, size=32):\n",
        "  srcstr=srcstr.split() \n",
        "  wordcnt=len(names.split())\n",
        "  wordrndnum=random.randrange(0, wordcnt-size)\n",
        "  return ' '.join(line for line in srcstr[wordrndnum:wordrndnum+size])"
      ],
      "execution_count": 353,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlZmPifCltNB"
      },
      "source": [
        "#from random import sample\n",
        "import random\n",
        "from IPython.display import clear_output\n",
        "\n",
        "MAX_LENGTH=200\n",
        "\n",
        "char_rnn = CharRNNCell()\n",
        "criterion = nn.NLLLoss()\n",
        "opt = torch.optim.Adam(char_rnn.parameters())\n",
        "history = []\n",
        "\n",
        "for i in range(1000):\n",
        "    opt.zero_grad()\n",
        "      \n",
        "    #batch_ix = to_matrix(sample(names, 32), max_len=MAX_LENGTH)\n",
        "    #подкладываю рандомные слова\n",
        "    batch_ix = to_matrix(getrndstr(names,MAX_LENGTH)) #new\n",
        "    batch_ix = torch.tensor(batch_ix, dtype=torch.int64)\n",
        "\n",
        "    logp_seq = rnn_loop(char_rnn, batch_ix)\n",
        "\n",
        "    # compute loss\n",
        "    predictions_logp = logp_seq[:, :-1]  # YOUR CODE HERE\n",
        "    actual_next_tokens = batch_ix[:, 1:]  # YOUR CODE HERE\n",
        "\n",
        "    #     print(predictions_logp.shape, actual_next_tokens.shape)\n",
        "    loss = criterion(\n",
        "        predictions_logp.contiguous().view(-1, num_tokens),\n",
        "        actual_next_tokens.contiguous().view(-1),\n",
        "    )\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    # train with backprop\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "    # HISTORY MOVED TO SEPARATE SNIPPET\n",
        "    history.append(loss.data.numpy())\n"
      ],
      "execution_count": 354,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gveyFkPkmkWh"
      },
      "source": [
        "**Generate samples**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ldv4OHXgmXR7"
      },
      "source": [
        "def generate_sample(char_rnn, seed_phrase=\" \", max_length=MAX_LENGTH, temperature=1.0):\n",
        "    \"\"\"\n",
        "    The function generates text given a phrase of length at least SEQ_LENGTH.\n",
        "    :param seed_phrase: prefix characters. The RNN is asked to continue the phrase\n",
        "    :param max_length: maximum output length, including seed_phrase\n",
        "    :param temperature: coefficient for sampling.  higher temperature produces more chaotic outputs,\n",
        "                        smaller temperature converges to the single most likely output\n",
        "    \"\"\"\n",
        "\n",
        "    x_sequence = [token_to_id[token] for token in seed_phrase]\n",
        "    x_sequence = torch.tensor([x_sequence], dtype=torch.int64)\n",
        "    hid_state = char_rnn.initial_state(batch_size=1)\n",
        "\n",
        "    # feed the seed phrase, if any\n",
        "    for i in range(len(seed_phrase) - 1):\n",
        "        hid_state, _ = char_rnn(x_sequence[:, i], hid_state)\n",
        "\n",
        "    # start generating\n",
        "    for _ in range(max_length - len(seed_phrase)):\n",
        "        hid_state, logits = char_rnn(x_sequence[:, -1], hid_state)\n",
        "        p_next = F.softmax(logits / temperature, dim=-1).data.numpy()[0]\n",
        "\n",
        "        # sample next token and push it back into x_sequence\n",
        "        next_ix = np.random.choice(num_tokens, p=p_next)\n",
        "        next_ix = torch.tensor([[next_ix]], dtype=torch.int64)\n",
        "        x_sequence = torch.cat([x_sequence, next_ix], dim=1)\n",
        "\n",
        "    return \"\".join([tokens[ix] for ix in x_sequence.data.numpy()[0]])"
      ],
      "execution_count": 313,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03cB5R-OoqpZ"
      },
      "source": [
        "**Generate text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBo81nfhPOvu"
      },
      "source": [
        "# An example of generated text. There is no function `generate_text` in the code above.\n",
        "# print(generate_text(length=500, temperature=0.2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaVIS8Qxmtf4",
        "outputId": "0e74639d-6491-41c6-f7f9-f3dc266a1e7f"
      },
      "source": [
        "for _ in range(3):\n",
        "    print(generate_sample(char_rnn))"
      ],
      "execution_count": 355,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " пругтарала он сечтой веско миали хругомой она строть пуса на свура присольстовном чить он бкажуди и мщепва скери кругы в шерлини корока ла нлюбок серусь что жо жек дноют тозжита чат нел на денепны по\n",
            " xxleqы моюшладае в он на ильцкает да тот до шедго бетитендной овнянает одмлогных о позгуть и нечни какая плои вых летской в прикновы и влугий в постре не хразнит лишиной онар вужноя смечасней жинний \n",
            " хорилый перебирянь и легоных тьxv брение и все бусной наши кради мижели страницся сневя не собою не в но совы нечать родиты севьеет они хоря тоть ивою срака ходнорый был корой омнете креди волкасный \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "UtVp8i6jm0di",
        "outputId": "2c6a24b8-67b2-48af-d89f-280201b69fe6"
      },
      "source": [
        "for _ in range(3):\n",
        "    print(generate_sample(char_rnn, seed_phrase=\" Al\"))"
      ],
      "execution_count": 356,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-356-8981a7a7b0b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar_rnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_phrase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\" Al\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-313-587194079e29>\u001b[0m in \u001b[0;36mgenerate_sample\u001b[0;34m(char_rnn, seed_phrase, max_length, temperature)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \"\"\"\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mx_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken_to_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseed_phrase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mx_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_sequence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mhid_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchar_rnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-313-587194079e29>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \"\"\"\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mx_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken_to_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseed_phrase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mx_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_sequence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mhid_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchar_rnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'A'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdYhz-tGnCfN",
        "outputId": "8843718d-5e3a-4195-9ba1-d4deb5c4eb21"
      },
      "source": [
        "for _ in range(3):\n",
        "    print(generate_sample(char_rnn, temperature=0.5))"
      ],
      "execution_count": 357,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " водой и скорой соли плодом нет ним с но породу на полего мило столит как дсев не и таня мема коленный в на сони слева тавно мила xi на безум не на я не вина и татьяна не другов и себен когом что когр\n",
            " подно другой ленили на стора свете мена сердце улоской он не старей кростой не подомно воздала даро в он он сердце долы как из на поэт на поэты не вы на сконной стравным в полет он сердце кокорой пос\n",
            " ум и тражицали веренный не следный всё и не всё он продол как тать не зародет саминий вас ленось на соловож он потовно ли в леник с рездней он порна помила ез не на посторов он продом заменный да вол\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXpgZ1JtjKB7"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1bYHnB9POvt"
      },
      "source": [
        "Plot the loss function (axis X: number of epochs, axis Y: loss function)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "l33YtZ0iPOvu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "f42ad435-60f0-4878-9b82-78c6c916a90b"
      },
      "source": [
        "import matplotlib.pyplot as plt \n",
        "# Your plot code here\n",
        "if (i + 1) % 100 == 0:\n",
        "    clear_output(True)\n",
        "    plt.plot(history, label=\"loss\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "#assert np.mean(history[:10]) > np.mean(history[-10:]), \"RNN didn't converge.\""
      ],
      "execution_count": 319,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD4CAYAAADFAawfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAWGklEQVR4nO3de5BkZX3G8ed3Tp+Z2SuX3akFWXGWiBeyCJgBtQirRaLgJaaM/iGVhEtQqlKJwWBMSVEpNTFFoikJuZRKKYiJF4hQiVkNFEF0ISHI7GaRZRdlBVZns7izi7tcdufSfX7545zTfXpmlum59Pa7Pd9PbUP36dPd7+m35+n3vP2+55i7CwAQrqjTBQAAvDSCGgACR1ADQOAIagAIHEENAIGrtONJV69e7QMDA+14agDoSps3b97n7v3T3deWoB4YGNDQ0FA7nhoAupKZ7TrSfXR9AEDgCGoACBxBDQCBa0sfNQDM18TEhIaHhzU6Otrpoiyovr4+rV27VkmStPwYghpAkIaHh7VixQoNDAzIzDpdnAXh7tq/f7+Gh4e1bt26lh9H1weAII2OjmrVqlVdE9KSZGZatWrVrPcSCGoAweqmkC7MZZuCCuq/v/cJff/HI50uBgAEJaig/tz3f6IHniCoAYRh+fLlnS6CpMCCOo5MEzVOZAAAZUEFdRJHqqZpp4sBAE3cXR/96Ee1fv16nXnmmbrtttskSXv27NGGDRt09tlna/369br//vtVq9V0+eWX19e94YYb5v36QQ3PiyNTLaVFDaDZJ//9MW3/v+cW9DnPeNlKffw3frmlde+8805t3bpVjzzyiPbt26dzzz1XGzZs0Ne+9jVddNFFuu6661Sr1XTo0CFt3bpVu3fv1rZt2yRJBw4cmHdZw2pR0/UBIEAPPPCALrnkEsVxrDVr1ujNb36zHn74YZ177rm65ZZb9IlPfEKPPvqoVqxYodNOO01PPvmkPvShD+muu+7SypUr5/36LbeozSyWNCRpt7u/a96vPF1h4ogWNYApWm35Hm0bNmzQpk2b9O1vf1uXX365rrnmGl166aV65JFHdPfdd+vzn/+8br/9dt18883zep3ZtKivlrRjXq82g0pkmqjRRw0gLBdccIFuu+021Wo1jYyMaNOmTTrvvPO0a9curVmzRh/84Af1gQ98QFu2bNG+ffuUpqne+9736lOf+pS2bNky79dvqUVtZmslvVPSX0q6Zt6veqTCxPRRAwjPe97zHj344IM666yzZGb69Kc/rZNOOkm33nqrPvOZzyhJEi1fvlxf+cpXtHv3bl1xxRVK84ER119//bxf39xnDkYz+6ak6yWtkPQn03V9mNlVkq6SpFNPPfVXdu064jGwj+jtN96vU45foi9eNjjrxwLoLjt27NBrX/vaThejLabbNjPb7O7Tht+MXR9m9i5Je91980ut5+43ufuguw/29097NpkZJbExPA8AJmmlj/p8Se82s6clfUPShWb2z+0oDMPzAGCqGYPa3a9197XuPiDp/ZK+6+6/047CJFHEj4kA6lrpmj3WzGWbghpHzY+JAAp9fX3av39/V4V1cTzqvr6+WT1uVjMT3f17kr43q1eYhTgyHRrvnkoBMHdr167V8PCwRka660BtxRleZiOoKeQJE14A5JIkmdVZULpZUF0fMRNeAGCKoII6oY8aAKYIKqjjKFKVoAaAJkEFdULXBwBMEVRQM+EFAKYKKqgrccTxqAFgkqCCOvsxka4PACgLKqjjyFSlRQ0ATYIK6uzktgQ1AJQFFdRxxGFOAWCyoIK6OLltNx2EBQDmK6igjqOsOPR+AEBDUEFdiU2SmPQCACVBBXWSBzWTXgCgIaigLro+GKIHAA1BBXXRombkBwA0BBXUcVQENS1qACgEFdRJ3vXBj4kA0BBUUBctan5MBICGoIK6MTyPoAaAQlBBncRZcWhRA0BDUEFddH3QRw0ADUEFNRNeAGCqoIK6PuGFcdQAUBdUUCfFOGp+TASAuqCCmgkvADBVUEFdiZnwAgCThRXUTHgBgCnCCmomvADAFEEFNRNeAGCqoIK68WMifdQAUAgqqBNOHAAAUwQV1DEnDgCAKYIK6iTix0QAmCyooOZ41AAwVVBBzYQXAJgqqKDm6HkAMFVQQc2xPgBgqhmD2sz6zOwHZvaImT1mZp9sV2EYngcAU1VaWGdM0oXu/oKZJZIeMLP/cPf/WejCRJHJjOF5AFA2Y1C7u0t6Ib+Z5Je2NXmTKGJ4HgCUtNRHbWaxmW2VtFfSPe7+0DTrXGVmQ2Y2NDIyMucCxZGpRosaAOpaCmp3r7n72ZLWSjrPzNZPs85N7j7o7oP9/f1zLlAlNlrUAFAyq1Ef7n5A0n2SLm5PcbIj6DE8DwAaWhn10W9mx+fXl0h6q6TH21WgODJ+TASAklZGfZws6VYzi5UF++3uvrFdBUoiY3geAJS0Murjh5LOOQplkZQdQY8JLwDQENTMRKkYnkfXBwAUggvqbHgeLWoAKAQX1JWYCS8AUBZcUCcxE14AoCy4oM6G59GiBoBCcEGdRBHD8wCgJLigZsILADQLLqgrjKMGgCbhBTUzEwGgSXhBHTPhBQDKwgtqJrwAQJPwgjqO6KMGgJLggjph1AcANAkuqGN+TASAJsEFNV0fANAsvKCOTFVGfQBAXXhBHdP1AQBl4QU1B2UCgCbhBXUcMeoDAEqCC+qEFjUANAkuqOMokruYnQgAueCCuhKbJNH9AQC58II6yoOakR8AICnEoI6zIhHUAJAJL6gjuj4AoCy8oK73UdOiBgApwKBOorzrg6AGAEkBBnVc/zGRrg8AkAIMaro+AKBZeEEdMeoDAMrCC2omvABAk/CCmgkvANAkvKAuJrzQogYASSEGNS1qAGgSblAz6gMAJIUY1DETXgCgLLygZsILADQJL6iZ8AIATcILaia8AECTGYPazF5uZveZ2XYze8zMrm5ngZjwAgDNKi2sU5X0EXffYmYrJG02s3vcfXtbCpT3UU/QogYASS20qN19j7tvya8/L2mHpFPaVaBi1EdKHzUASJplH7WZDUg6R9JD7SiMVGpR0/UBAJJmEdRmtlzSHZI+7O7PTXP/VWY2ZGZDIyMjcy5QEdQ1WtQAIKnFoDazRFlIf9Xd75xuHXe/yd0H3X2wv79/zgUqRn3QRw0AmVZGfZikL0na4e6fbXeB4rhoUdP1AQBSay3q8yX9rqQLzWxrfnlHuwrEsT4AoNmMw/Pc/QFJdhTKIomj5wHAZMHNTIxpUQNAk+CC2sxUiYw+agDIBRfUUtaqpusDADJBBnUSR3R9AEAuyKDOWtR0fQCAFGhQVyKjRQ0AuTCDOjamkANALsygjiKmkANALsygjhmeBwCFIIM6po8aAOqCDOoK46gBoC7QoGYcNQAUwgzq2Di5LQDkwgzqiOF5AFAINKgj+qgBIBdkUGejPuj6AAAp0KDO+qhpUQOAFGpQMzwPAOqCDOqY4XkAUBdkUCdMIQeAuiCDmjO8AEBDkEHNGV4AoCHIoI6Z8AIAdUEGdSUyTXAqLgCQFGpQc4YXAKgLM6ijiBY1AOSCDGr6qAGgIcigZgo5ADSEGdScigsA6gIN6ki11OVOWANAoEFtkkQ/NQAo0KCO4yyo6f4AgECDOomyYhHUABBoUMd510eVsdQAEGZQV+j6AIC6MIM67/rgx0QACDaosxY108gBINSgjhmeBwCFIIO6/mMiQQ0AYQZ10UfN6bgAoIWgNrObzWyvmW07GgWSyqM+6KMGgFZa1F+WdHGby9GkUh9HTYsaAGYManffJOnZo1CWOvqoAaBhwfqozewqMxsys6GRkZF5PVcSM44aAAoLFtTufpO7D7r7YH9//7yeiynkANAQ5KiPhCnkAFAXZFAXw/OYmQgArQ3P+7qkByW92syGzezKdheqp5IVa7xKUANAZaYV3P2So1GQst48qMcIagAIs+uDFjUANAQd1GP0UQNAmEHdW4klSWMTtQ6XBAA6L9Cgzrs+aFEDQJhB3RPTRw0AhSCDOopMSWyM+gAABRrUUtaqpkUNACEHdYWgBgAp4KDurcQaqzLqAwCCDWpa1ACQCTuoGZ4HAOEGdW8l0tgEQQ0AwQY1LWoAyIQb1DEtagCQAg7q3iTmoEwAoICDmgkvAJAJNqh7KxHjqAFAgQc1LWoACDmok1ijHI8aAMIN6pV9FT03Wu10MQCg48IN6iWJxqsprWoAi164Qd2XnSD9eVrVABa5cIN6SSJJem50osMlAYDOCjeo+/KgPkxQA1jcwg3qJVnXBz8oAljswg3qvEX9PF0fABa5cIO66KM+TIsawOIWbFCv6Cu6PmhRA1jcgg3qJUmsJUmsvc+NdbooANBRwQa1mekVq5bqp8++2OmiAEBHBRvUkjSwapme3n+o08UAgI4KOqhfsXqpfrr/kGqpd7ooANAxQQf1aauXabyW6ql9dH8AWLyCDurzX7laknTf43s7XBIA6Jygg3rtCUv1mpNW6I4tw3R/AFi0gg5qSfr9t/ySHn/med1wz4/lTlgDWHwqnS7ATN591sv0Xzv36R/u26kfPPWsLlp/ks4bOFGnrlqqlX0VmVmniwgAbRV8UJuZrv+t1+mMk1fqlv9+Wn+xcXv9viVJrDUre7WiL1ESmypxpN5KpN5KrN4kUk8cqRKZkkp2vVinEpncJZfLZKrEpiSOFEeWrR9Hiix77TgyxWYyU3Y9MkXW+H+UL4/MpOxf/csjuy6ZsscXy5QvU/3+7DHldSxfsXmZ1e/rqUSq1rI9jDgy9VYi1VLXi+M19SWRKlG2nRO1VNVJ3Ubl77aiHFOXT66HplstPsamve+lXj/OyxzlK/UlscZrqdxdPZVIcqmaulyq72HFkalac6XuWpLESj17rul2wIo6jywrVLXmMpN6K7HiyDRWrWmi5orNVHOfdi8uyuuqeP/L2+MuRVG2VcV6kZl6k0juUloqu5ce43Ll/5qXSerJP5tFNU5+rJlKn+nsfjNTNU1VS109caTITNXUFeXlmVyGNHXFkamnEil1qVpL5cqeV1L9MxRZY9tcXt/eOGrUZOMza023X6r+y5/zaur1rs7y38+LY1W5sr/7mnv971uSnj00rjs279aZpxynV61Zrr3Pj+mEZT3qiSONTtS0rLeiJDbVUlcUmZIoUjVNZZb9zdfy7TfL3m8z04FD4/rzjdt1wemr9YZ1q7Tn4KheHKvqxGU9Wr28V5U4f8/z99FMWtpT0fLehY9Va0d3wuDgoA8NDS3480rSnoOHtWXXAe05eFjPHBzVz58f0/OjE1kg1Vxj1TS/1FStuSZqaX5pXK+mXg8+dxfd3wAKkWVfBmamF8Zmd6yhVct6tPnP3jqn1zWzze4+ON19LUW/mV0s6UZJsaQvuvtfzakkC+Dk45bona9bsqDPmaauauqqplmgV2up0vybspYHeZp/y9fcs+vuSlMp9Wx56kVLqXhWL7Vw8iWldeptmdI6xbJyyyhfJV/P68/1wlhVS5JYklRLXeP5F9XxSxONVbMvo2otVZLvSdRLVfpSKn8/NS9v/uY68mOO/A13pOdrWj7p4cV7m8RR/T0dnagpjiP1xKbxatYCKrbH8l2YbLkUm2kibzUW909u7RYtbVdWb0kcySSNVbMv8d4kVm8caaxaU08lmrZrrWgVF605r/9HGqvWVMn3yGqpVE1TSaqXPZq0B1V/9tLe0uRWZtEIiaNGWcqPdTX2DIpWfJq64rzFOV5NlbqrkrfKU/emMhSf/55KpLGJVFG+Z1nsNRR7OsV7mHrjc2imel2V67Tc4i9/Bqbdy5n02CjK6njy309fktXVwcNVLe2JNZFm74vlr/eqNSv0i0PjmqilWtmX6ODhCZlle2WHxqrZnlJkSt01USvej2zbK1G2x+HuGp1IdWi8psMTVV34mjU6PFHTi2NVnXRcn5b3VrT/hXHtf3FMtdSb9nKzvZL2/Ow3Y1CbWSzpHyW9VdKwpIfN7Fvuvv2lH3nsiCJTT2TqCf+3VQCLUCvJdJ6kne7+pLuPS/qGpN9sb7EAAIVWgvoUST8r3R7OlzUxs6vMbMjMhkZGRhaqfACw6C3Yvr673+Tug+4+2N/fv1BPCwCLXitBvVvSy0u31+bLAABHQStB/bCk081snZn1SHq/pG+1t1gAgMKMoz7cvWpmfyjpbmXD825298faXjIAgKQWx1G7+3ckfafNZQEATIOBwwAQuLZMITezEUm75vjw1ZL2LWBxjgVs8+LANne/+WzvK9x92iFzbQnq+TCzoSPNd+9WbPPiwDZ3v3ZtL10fABA4ghoAAhdiUN/U6QJ0ANu8OLDN3a8t2xtcHzUAoFmILWoAQAlBDQCBCyaozexiM/uRme00s491ujwLxcxebmb3mdl2M3vMzK7Ol59oZveY2RP5/0/Il5uZ/V3+PvzQzF7f2S2YOzOLzex/zWxjfnudmT2Ub9tt+bFjZGa9+e2d+f0DnSz3XJnZ8Wb2TTN73Mx2mNmbur2ezeyP88/1NjP7upn1dVs9m9nNZrbXzLaVls26Xs3ssnz9J8zsstmUIYigLp1F5u2SzpB0iZmd0dlSLZiqpI+4+xmS3ijpD/Jt+5ike939dEn35rel7D04Pb9cJelzR7/IC+ZqSTtKt/9a0g3u/kpJv5B0Zb78Skm/yJffkK93LLpR0l3u/hpJZynb9q6tZzM7RdIfSRp09/XKjgX0fnVfPX9Z0sWTls2qXs3sREkfl/QGZSdj+XgR7i3x/EzLnbxIepOku0u3r5V0bafL1aZt/TdlpzX7kaST82UnS/pRfv0Lki4prV9f71i6KDsc7r2SLpS0Udkp/vZJqkyuc2UH/HpTfr2Sr2ed3oZZbu9xkp6aXO5urmc1TipyYl5vGyVd1I31LGlA0ra51qukSyR9obS8ab2ZLkG0qNXiWWSOdfmu3jmSHpK0xt335Hc9I2lNfr1b3ou/lfSnktL89ipJB9y9OK1zebvq25zffzBf/1iyTtKIpFvy7p4vmtkydXE9u/tuSX8j6aeS9iirt83q7nouzLZe51XfoQR11zOz5ZLukPRhd3+ufJ9nX7FdM07SzN4laa+7b+50WY6iiqTXS/qcu58j6UU1docldWU9n6Ds/KnrJL1M0jJN7SLoekejXkMJ6q4+i4yZJcpC+qvufme++OdmdnJ+/8mS9ubLu+G9OF/Su83saWUnQ75QWf/t8WZWHFq3vF31bc7vP07S/qNZ4AUwLGnY3R/Kb39TWXB3cz3/uqSn3H3E3Sck3ams7ru5nguzrdd51XcoQd21Z5ExM5P0JUk73P2zpbu+Jan45fcyZX3XxfJL81+P3yjpYGkX65jg7te6+1p3H1BWl99199+WdJ+k9+WrTd7m4r14X77+MdXydPdnJP3MzF6dL/o1SdvVxfWsrMvjjWa2NP+cF9vctfVcMtt6vVvS28zshHxP5G35stZ0upO+1Ln+Dkk/lvQTSdd1ujwLuF2/qmy36IeStuaXdyjrm7tX0hOS/lPSifn6pmwEzE8kParsF/WOb8c8tv8tkjbm10+T9ANJOyX9i6TefHlffntnfv9pnS73HLf1bElDeV3/q6QTur2eJX1S0uOStkn6J0m93VbPkr6urA9+Qtme05VzqVdJv5dv+05JV8ymDEwhB4DAhdL1AQA4AoIaAAJHUANA4AhqAAgcQQ0AgSOoASBwBDUABO7/AejEIvYRSsmAAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUosnNZEPOvv"
      },
      "source": [
        "### More poetic model\n",
        "\n",
        "Let's use LSTM instead of vanilla RNN and compare the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7p6YsLpPOvv"
      },
      "source": [
        "Plot the loss function of the number of epochs. Does the final loss become better?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "KZeWrdlUPOvv"
      },
      "source": [
        "# Your beautiful code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHtfrgBoPOvw"
      },
      "source": [
        "Generate text using the trained net with different `temperature` parameter: `[0.1, 0.2, 0.5, 1.0, 2.0]`.\n",
        "\n",
        "Evaluate the results visually, try to interpret them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "5DiP7G2aPOvw"
      },
      "source": [
        "# Text generation with different temperature values here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XSgbUlCPOvx"
      },
      "source": [
        "### Saving and loading models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khe9kdFMPOvx"
      },
      "source": [
        "Save the model to the disk, then load it and generate text. Examples are available [here](https://pytorch.org/tutorials/beginner/saving_loading_models.html])."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "WhH2gmdtPOvx"
      },
      "source": [
        "# Saving and loading code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLhhvurXPOvx"
      },
      "source": [
        "### References\n",
        "1. <a href='http://karpathy.github.io/2015/05/21/rnn-effectiveness/'> Andrew Karpathy blog post about RNN. </a> \n",
        "There are several examples of genration: Shakespeare texts, Latex formulas, Linux Sourse Code and children names.\n",
        "2. <a href='https://github.com/karpathy/char-rnn'> Repo with char-rnn code </a>\n",
        "3. Cool repo with PyTorch examples: [link](https://github.com/spro/practical-pytorch`)"
      ]
    }
  ]
}