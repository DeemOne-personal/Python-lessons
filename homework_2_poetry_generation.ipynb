{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "colab": {
      "name": "homework_2_poetry_generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeemOne-personal/Python-lessons/blob/main/homework_2_poetry_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Quao1WwFPOvn"
      },
      "source": [
        "## Homework №2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txzufAHoPOvo"
      },
      "source": [
        "### Almost Shakespeare\n",
        "\n",
        "Let's try to generate some Shakespeare poetry using RNNs. The sonnets file is available in the notebook directory.\n",
        "\n",
        "Text generation can be designed in several steps:\n",
        "    \n",
        "1. Data loading.\n",
        "2. Dictionary generation.\n",
        "3. Data preprocessing.\n",
        "4. Model (neural network) training.\n",
        "5. Text generation (model evaluation).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUPTdS1BPOvp"
      },
      "source": [
        "### Data loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KyYA5dZPOvp"
      },
      "source": [
        "Shakespeare sonnets are awailable at this [link](http://www.gutenberg.org/ebooks/1041?msg=welcome_stranger). In addition, they are stored in the same directory as this notebook (`sonnetes.txt`).\n",
        "\n",
        "Simple preprocessing is already done for you in the next cell: all technical info is dropped."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbBhZ03lmR4U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71566a1e-441c-4e36-e69e-8931b3f9cd85"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "hERfiZL8POvq"
      },
      "source": [
        "#with open('/content/drive/MyDrive/Colab Notebooks/Edu4 - DS MFTI Adv/Files/HW2/onegin.txt', 'r') as iofile:  \n",
        "with open('/content/drive/MyDrive/Colab Notebooks/Edu4 - DS MFTI Adv/Files/HW2/sonnets.txt', 'r') as iofile:\n",
        "    source_text = iofile.readlines()\n",
        "    \n",
        "TEXT_START = 45\n",
        "TEXT_END = -368\n",
        "text = source_text[TEXT_START : TEXT_END]\n",
        "assert len(text) == 2616"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1j7isOykPOvq"
      },
      "source": [
        "In opposite to the in-class practice, this time we want to predict complex text. Let's reduce the complexity of the task and lowercase all the symbols.\n",
        "\n",
        "Now variable `text` is a list of strings. Join all the strings into one and lowercase it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcsAzqvIPOvr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52e73068-243e-4226-fc8d-8186d593daa0"
      },
      "source": [
        "import string \n",
        "# Join all the strings into one and lowercase it\n",
        "# Put result into variable text.\n",
        "#res=''.join([line.rstrip('\\n') for line in text]).replace(',','').replace('  ', ' ').lstrip()\n",
        "text=''.join(line for line in text).lower()\n",
        "\n",
        "# Your great code here\n",
        "assert len(text) == 100225, 'Are you sure you have concatenated all the strings?'\n",
        "assert not any([x in set(text) for x in string.ascii_uppercase]), 'Uppercase letters are present'\n",
        "print('OK!')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OK!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZjUQri8POvr"
      },
      "source": [
        "Put all the characters, that you've seen in the text, into variable `tokens`.\n",
        "\n",
        "Create dictionary `token_to_idx = {<char>: <index>}` and dictionary `idx_to_token = {<index>: <char>}`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "G2bnKUnZPOvs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67a7b2f1-7a9d-48ad-b575-594699da39d1"
      },
      "source": [
        "#put all characters to variable tokens \n",
        "tokens = sorted(set(text))\n",
        "num_tokens = len(tokens)\n",
        "print('num tokens: '+str(num_tokens))\n",
        "\n",
        "# dict <index>:<char>\n",
        "# Your great code here\n",
        "idx_to_token={k:v for k, v in enumerate(tokens)}\n",
        "\n",
        "# dict <char>:<index>\n",
        "# Your great code here\n",
        "#token_to_idx={v:k for k, v in enumerate(tokens)}\n",
        "token_to_idx = {token: idx for idx, token in enumerate(tokens)}\n",
        "\n",
        "#check the size\n",
        "assert num_tokens == len(idx_to_token), \"dictionaries must have same size\"\n",
        "assert num_tokens == len(token_to_idx), \"dictionaries must have same size\"\n",
        "\n",
        "#check positional assignments of two arrays\n",
        "for n in range(len(tokens)):\n",
        "  assert token_to_idx[idx_to_token[n]]==n, 'positional assign of two arrays must be the same'\n",
        "\n",
        "#setting max length variable\n",
        "#MAX_LENGTH=max(map(len,str(text).split('\\n')))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num tokens: 38\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCT6SDxrPOvs"
      },
      "source": [
        "*Comment: in this task we have only 38 different tokens, so let's use one-hot encoding.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-j5uf_-POvt"
      },
      "source": [
        "### Building the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79NJaVc_BNOh"
      },
      "source": [
        "#text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4go5JugdPOvt"
      },
      "source": [
        "Now we want to build and train recurrent neural net which would be able to something similar to Shakespeare's poetry.\n",
        "\n",
        "Let's use vanilla RNN, similar to the one created during the lesson."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gODFS4klYhyS"
      },
      "source": [
        "import numpy as np\n",
        "def to_matrix(names, max_len=None, pad=token_to_idx[\" \"], dtype=\"int32\", batch_first=True):\n",
        "    \"\"\"Casts a list of names into rnn-digestable matrix\"\"\"\n",
        "\n",
        "    max_len = max_len or max(map(len, names))\n",
        "    names_ix = np.zeros([len(names), max_len], dtype) + pad\n",
        "\n",
        "    for i in range(len(names)):\n",
        "        line_ix = [token_to_idx[c] for c in names[i]]\n",
        "        names_ix[i, : len(line_ix)] = line_ix\n",
        "\n",
        "    if not batch_first:  # convert [batch, time] into [time, batch]\n",
        "        names_ix = np.transpose(names_ix)\n",
        "\n",
        "    return names_ix"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DP_eAMleFYa",
        "outputId": "808c6c08-8105-442a-ad88-2abbcce0b0cc"
      },
      "source": [
        "#matrix conversion result:\n",
        "ttt=text[0:23]\n",
        "#print(tttmatrix)\n",
        "textmatrix=to_matrix(ttt)\n",
        "textmatrix"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1],\n",
              "       [ 1],\n",
              "       [17],\n",
              "       [29],\n",
              "       [26],\n",
              "       [24],\n",
              "       [ 1],\n",
              "       [17],\n",
              "       [12],\n",
              "       [20],\n",
              "       [29],\n",
              "       [16],\n",
              "       [30],\n",
              "       [31],\n",
              "       [ 1],\n",
              "       [14],\n",
              "       [29],\n",
              "       [16],\n",
              "       [12],\n",
              "       [31],\n",
              "       [32],\n",
              "       [29],\n",
              "       [16]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTsTNtlvdi0k",
        "outputId": "00aed62d-cc6c-4c71-8f38-24234fd77990"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[15, 16, 25,  1,  1],\n",
            "        [21, 19, 26, 25,  1]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5S2_1BSf-VV"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "emb_test = nn.Embedding(55, 7) # One hot + Linears\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "et0AShLGbWwr"
      },
      "source": [
        "#convert to tensor sample\n",
        "example_batch = torch.from_numpy(to_matrix(['den ', 'jhon '])).type(torch.int64)\n",
        "print('***Convert to tensor sample:')\n",
        "print(example_batch.shape)\n",
        "print(example_batch)\n",
        "\n",
        "print('***Embedding with parameters sample:')\n",
        "example_batch = torch.from_numpy(to_matrix(['den ', 'jhon '])).type(torch.int64)\n",
        "emb = emb_test(example_batch)\n",
        "print(emb.shape)\n",
        "emb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVyc7GR8u03_"
      },
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from random import sample"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GuBfUBLr3Ei"
      },
      "source": [
        "class CharRNNLoop(nn.Module):\n",
        "    def __init__(self, num_tokens=num_tokens, emb_size=16, rnn_num_units=64):\n",
        "        super(self.__class__, self).__init__()\n",
        "        self.emb = nn.Embedding(num_tokens, emb_size)\n",
        "        self.rnn = nn.LSTM(emb_size, rnn_num_units, batch_first=True)\n",
        "        self.hid_to_logits = nn.Linear(rnn_num_units, num_tokens)\n",
        "        self.num_units = rnn_num_units\n",
        "\n",
        "    def forward(self, x):\n",
        "        h_seq, _ = self.rnn(self.emb(x))\n",
        "        next_logits = self.hid_to_logits(h_seq)\n",
        "        next_logp = F.log_softmax(next_logits, dim=-1)\n",
        "        return next_logp\n",
        "    \n",
        "    def initial_state(self, batch_size):\n",
        "        \"\"\" return rnn state before it processes first input (aka h0) \"\"\"\n",
        "        return torch.zeros(batch_size, self.num_units, requires_grad=True)"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PorCDw--4sE"
      },
      "source": [
        "#added by me\n",
        "#char_rnn = CharRNNCell()\n",
        "char_rnn = CharRNNLoop() #replace CharRNNCell \n",
        "criterion = nn.NLLLoss() "
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZWvYgL9sPxk"
      },
      "source": [
        "MAX_LENGTH=max(map(len,str(text).split('\\n')))\n",
        "\n",
        "model = CharRNNLoop()\n",
        "opt = torch.optim.Adam(model.parameters())\n",
        "history = []\n",
        "\n",
        "# the model applies over the whole sequence\n",
        "batch_ix = to_matrix(sample(text, 32), max_len=MAX_LENGTH)\n",
        "batch_ix = torch.LongTensor(batch_ix)"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFl4KIhLsUGs"
      },
      "source": [
        "logp_seq = model(batch_ix)\n",
        "\n",
        "loss = criterion(\n",
        "    logp_seq[:, :-1].contiguous().view(-1, num_tokens), batch_ix[:, 1:].contiguous().view(-1)\n",
        ")\n",
        "\n",
        "loss.backward()"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "dCsQRVX4sW41",
        "outputId": "18663ca6-89fb-4e98-f782-abc9a5c4d2db"
      },
      "source": [
        "#added by me\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "#/added by me\n",
        "\n",
        "MAX_LENGTH = 16\n",
        "\n",
        "for i in range(500):\n",
        "    batch_ix = to_matrix(sample(text, 32), max_len=MAX_LENGTH)\n",
        "    batch_ix = torch.tensor(batch_ix, dtype=torch.int64)\n",
        "\n",
        "    logp_seq = model(batch_ix)\n",
        "\n",
        "    predictions_logp = logp_seq[:, :-1]  # YOUR CODE HERE\n",
        "    actual_next_tokens = batch_ix[:, 1:]  # YOUR CODE HERE\n",
        "\n",
        "    loss = criterion(\n",
        "        predictions_logp.contiguous().view(-1, num_tokens), actual_next_tokens.contiguous().view(-1)\n",
        "    )\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    # train with backprop\n",
        "    # YOUR CODE HERE\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    opt.zero_grad()\n",
        "\n",
        "    history.append(loss.data.numpy())\n",
        "    if (i + 1) % 100 == 0:\n",
        "        clear_output(True)\n",
        "        plt.plot(history, label=\"loss\")\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "assert np.mean(history[:10]) > np.mean(history[-10:]), \"RNN didn't converge.\""
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "4nYjs-XYDpgJ",
        "outputId": "c00c7493-f6b0-4ca3-bdb8-14b48c926033"
      },
      "source": [
        "#!!!!!!!!!ОШИБКА ЗДЕСЬ\n",
        "#sample generation\n",
        "def generate_sample(char_rnn, seed_phrase=\" \", max_length=MAX_LENGTH, temperature=1.0):\n",
        "    \"\"\"\n",
        "    The function generates text given a phrase of length at least SEQ_LENGTH.\n",
        "    :param seed_phrase: prefix characters. The RNN is asked to continue the phrase\n",
        "    :param max_length: maximum output length, including seed_phrase\n",
        "    :param temperature: coefficient for sampling.  higher temperature produces more chaotic outputs,\n",
        "                        smaller temperature converges to the single most likely output\n",
        "    \"\"\"\n",
        "\n",
        "    x_sequence = [token_to_idx[token] for token in seed_phrase]\n",
        "    x_sequence = torch.tensor([x_sequence], dtype=torch.int64)\n",
        "    hid_state = char_rnn.initial_state(batch_size=1)\n",
        "\n",
        "    # feed the seed phrase, if any\n",
        "    for i in range(len(seed_phrase) - 1):\n",
        "        #hid_state, _ = char_rnn(x_sequence[:, i], hid_state)\n",
        "        hid_state, _ = char_rnn(x_sequence[:, i])\n",
        "\n",
        "    # start generating\n",
        "    for _ in range(max_length - len(seed_phrase)):\n",
        "        #hid_state, logits = char_rnn(x_sequence[:, -1], hid_state)      \n",
        "        hid_state, logits = char_rnn(x_sequence[:, -1])      \n",
        "        p_next = F.softmax(logits / temperature, dim=-1).data.numpy()[0]\n",
        "\n",
        "        # sample next token and push it back into x_sequence\n",
        "        next_ix = np.random.choice(num_tokens, p=p_next)\n",
        "        next_ix = torch.tensor([[next_ix]], dtype=torch.int64)\n",
        "        x_sequence = torch.cat([x_sequence, next_ix], dim=1)\n",
        "\n",
        "    return \"\".join([tokens[ix] for ix in x_sequence.data.numpy()[0]])\n",
        "\n",
        "\n",
        "for _ in range(10):\n",
        "    print(generate_sample(model, seed_phrase='hello', temperature=1.0))"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-104-e72412699171>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_phrase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'hello'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-104-e72412699171>\u001b[0m in \u001b[0;36mgenerate_sample\u001b[0;34m(char_rnn, seed_phrase, max_length, temperature)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed_phrase\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m#hid_state, _ = char_rnn(x_sequence[:, i], hid_state)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mhid_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchar_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_sequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# start generating\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-96-02ea2a4415c7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mh_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mnext_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhid_to_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mnext_logp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    618\u001b[0m                            \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m                            ):\n\u001b[0;32m--> 620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m         self.check_hidden_size(hidden[0], self.get_expected_hidden_size(input, batch_sizes),\n\u001b[1;32m    622\u001b[0m                                'Expected hidden[0] size {}, got {}')\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    201\u001b[0m             raise RuntimeError(\n\u001b[1;32m    202\u001b[0m                 'input must have {} dimensions, got {}'.format(\n\u001b[0;32m--> 203\u001b[0;31m                     expected_input_dim, input.dim()))\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             raise RuntimeError(\n",
            "\u001b[0;31mRuntimeError\u001b[0m: input must have 3 dimensions, got 2"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPOrk2xor3_Y"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1bYHnB9POvt"
      },
      "source": [
        "Plot the loss function (axis X: number of epochs, axis Y: loss function)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "l33YtZ0iPOvu"
      },
      "source": [
        "# Your plot code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBo81nfhPOvu",
        "outputId": "fbffc005-5bf0-4ddc-9fe7-3da7cc9878b0"
      },
      "source": [
        "# An example of generated text. There is no function `generate_text` in the code above.\n",
        "# print(generate_text(length=500, temperature=0.2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hide my will in thine?\n",
            "  shall will in of the simend that in my sime the seave the seave the sorll the soren the sange the seall seares and and the fart the wirl the seall the songh whing that thou hall will thoun the soond beare the with that sare the simest me the fart the wirl the songre the with thy seart so for shat so for do the dost the sing the sing the sing the soond canding the sack and the farling the wirl of sore sich and that with the seare the seall so fort the with the past the wirl the simen the wirl the sores the sare\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUosnNZEPOvv"
      },
      "source": [
        "### More poetic model\n",
        "\n",
        "Let's use LSTM instead of vanilla RNN and compare the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7p6YsLpPOvv"
      },
      "source": [
        "Plot the loss function of the number of epochs. Does the final loss become better?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "KZeWrdlUPOvv"
      },
      "source": [
        "# Your beautiful code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHtfrgBoPOvw"
      },
      "source": [
        "Generate text using the trained net with different `temperature` parameter: `[0.1, 0.2, 0.5, 1.0, 2.0]`.\n",
        "\n",
        "Evaluate the results visually, try to interpret them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "5DiP7G2aPOvw"
      },
      "source": [
        "# Text generation with different temperature values here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XSgbUlCPOvx"
      },
      "source": [
        "### Saving and loading models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khe9kdFMPOvx"
      },
      "source": [
        "Save the model to the disk, then load it and generate text. Examples are available [here](https://pytorch.org/tutorials/beginner/saving_loading_models.html])."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "WhH2gmdtPOvx"
      },
      "source": [
        "# Saving and loading code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLhhvurXPOvx"
      },
      "source": [
        "### References\n",
        "1. <a href='http://karpathy.github.io/2015/05/21/rnn-effectiveness/'> Andrew Karpathy blog post about RNN. </a> \n",
        "There are several examples of genration: Shakespeare texts, Latex formulas, Linux Sourse Code and children names.\n",
        "2. <a href='https://github.com/karpathy/char-rnn'> Repo with char-rnn code </a>\n",
        "3. Cool repo with PyTorch examples: [link](https://github.com/spro/practical-pytorch`)"
      ]
    }
  ]
}